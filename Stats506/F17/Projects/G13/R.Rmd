---
title: "RIDGE REGRESSION | R"
author: "Group 13: Ruikun Xiao, Reed Coots, Yuzhe Ye"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.align = 'center', fig.width = 4, fig.height = 3)
set.seed(42)
```

### Introduction | <a href="home.html">Home</a>

Ridge regression method is one of the so-called "shrinkage methods", which is usually applied to a regression model when there is instablity resulting from collinearity of predictors.

In ordinary linear regression (OLS), the estimates of coefficient $\beta$ are given by:
$$\hat\beta=(X^TX)^{-1}X^Ty.$$
When the predictors are collinear or almost collinear, the matrix $X^TX$ here becomes singular (rarely the case) or almost singular, then the inverse would respond sensitively to errors, which results in instability of prediction with such a model.

Ridge regression, however, make a trade-off between bias and variance in prediction. By introducing a relatively small bias, you may expect a large reduction in the variance, and thus in the mean-squared error:
$$MSE=E(\hat\beta-\beta)^2=(E(\hat\beta-\beta))^2+E(\hat\beta-E\hat\beta)^2=\mathrm{bias^2+variance}.$$
This is achieved by introducing a penalty term into the loss function:
$$(y-X\beta)^T(y-X\beta)+\lambda\sum_j\beta_j^2.$$
The $\beta$ that minimizes the new loss function is the ridge regression estimate of $\beta$:
$$\hat\beta=(X^TX+\lambda I)^{-1}X^Ty.$$
It is clearly biased as $E(\hat\beta)=(X^TX+\lambda I)^{-1}(X^TX)\beta$ (shrinking the coefficients towards $0$), but note that $(X^TX)^{-1}$ is replaced by $(X^TX+\lambda I)^{-1}$ here, which should be less unstable.

### The Example Dataset

In the following sections, the method would be illutrated with dataset ``faraway:seatpos`` in R, where the response ``hipcenter`` is modified to take the absolute values.

```{r}
library(faraway)
data(seatpos)
seatpos$hipcenter = -seatpos$hipcenter
head(seatpos)
```

The condition number of $X^TX$ (where $X$ is the design matrix) is large, suggesting strong collinearity of the predictors:

```{r}
X = as.matrix(seatpos[, -9])
e = eigen(t(X) %*% X)$val
max(e)/min(e)
```

### Fitting a Ridge Regression Model

As the beginning of ridge regression, it is recommended to standardize the predictors. You can still carry out ridge regression without doing so, but standardization would improve the effect of ridge regression, as it makes the shrinking fair to each coefficients. Luckily, the function that we are going to use here automatically standardizes the data, so we don't need to do the standardization by ourselves.

To carry out ridge regression in R, you will need function ``lm.ridge`` in package ``MASS``. (The function ```penalized``` in package ```penalized``` should also work.)

```{r}
library(MASS)
fit = lm.ridge(hipcenter ~ ., seatpos, lambda = seq(0, .4, 1e-3))
```

We can observe how the coefficients shrink as $\lambda$ grows larger:

```{r}
par(mar = c(4, 4, 0, 0), cex = 0.7, las = 1)
matplot(fit$lambda, coef(fit), type = "l", ylim = c(-1, 3),
        xlab = expression(lambda), ylab = expression(hat(beta)))
```

The fitted model is an object of class ```"ridgelm"```. There are many attributes: note that with ```xm``` and ```scale``` you can extract the means and the scales that the input values are centered and scaled with.

### Selection of $\lambda$

As $\lambda$ grows larger, the coefficients (as well as prediction variances) shirnk, while the bias increases. Thus we have to select a $\lambda$ to make a trade-off, so as to control the overall prediction error. However, this is not automatically given to us as the range of ```lambda``` in the codes is input as an argument. So, though we can work out the best $\lambda$ with package ```MASS```, we still need a range to start with.

Notice that the eigenvalues of $X^TX+\lambda I$ should be a good reference, which can be obtained by adding $\lambda$ to the eigenvalues of $X^TX$:

```{r}
X = sapply(seatpos[, -9], scale)
round(eigen(t(X) %*% X)$val, 3)
```

So in order to reduce collinearity (i.e. reduce the difference in ratio among the eigenvalues of $X^TX+\lambda I$) without introducing too much bias, $10^1\sim10^2$ seems be a good point to start with.

```{r}
fit = lm.ridge(hipcenter ~ ., seatpos, lambda = seq(0, 30, 1e-3))
```

Then, to select an appropriate $\lambda$, we can apply the command ```select```:

```{r}
select(fit)
```

You may want to specify the functon with ```MASS::select``` to avoid mistakenly applying ```dplyr::MASS```.

Here ```HKB estimator``` and ```L-W estimator``` refer to different estimate of the ridge constant $\phi$ (which means that as a rough estimate, you can also take $\hat\beta_{ridge}=\hat\phi\hat\beta_{OLS}$), while the location of the smallest value of GCV (generalized cross validation) score provides a suggestion for $\lambda$.

```{r}
const = as.numeric(names(which.min(fit$GCV)))
```

To check how the value of the GCV score varies, you can also plot the GCV scores with
```{r}
par(mar = c(4, 4, 0, 0), cex = 0.7, las = 1)
plot(names(fit$GCV), fit$GCV, type = 'l',
     xlab = expression(lambda), ylab = "GCV Score")
```

You can also plot in-sample errors vs $\lambda$ to get more information before selection of $\lambda$.

### Prediction with a Fitted Ridge Regression Model

In package ```MASS```, we don't have established function for prediction. Thus we will have to work out the predicted values explicitly.

For later convenience (to assess the out-of-sample prediction performance of ridge regression with this dataset), in this section we will split the dataset into training data and test data:

```{r}
test_obs = sample(nrow(seatpos), 5)
test = seatpos[test_obs, ]
training = seatpos[-test_obs, ]
test
```

Fit the model with training data:

```{r}
trained = lm.ridge(hipcenter ~ ., training, lambda = seq(0, 30, 1e-3))
select(trained)
```

Work out the predicted values with fitted coeficients and scaled predictors:

```{r}
predicted = trained$ym +
  scale(test[, -9], center = trained$xm, scale = trained$scales) %*%
  trained$coef[, which.min(trained$GCV)]
as.numeric(predicted)
```

It is not fitting perfectly well, in fact. To assess its performance, you can work out the square-root of MSE, i.e. RMSE:

```{r}
sqrt(mean((predicted - test$hipcenter)^2))
```

### Comparison with OLS

In terms of in-sample error, it has been proved by Hoerl and Kennard (1970) that with an appropraite value of $\lambda$, ridge regression can always preform better than OLS, i.e. having smaller MSE.

However, the method of ridge regression does not necessarily promise a better performance in terms of out-of-sample errors.

```{r}
fit.OLS = lm(hipcenter ~ ., training)
sqrt(mean((predict(fit.OLS, test) - test$hipcenter)^2))
```

It is likely to be even smaller than that of ridge regression in this case. And it seems that ridge regression is not having a good performance on this data set.

### Reflection

The size of this data set is too small, and there may be many factors resulting in the bad performance of ridge regression on it. It is a pity that we can not present the power of ridge regression in a clear and impressive way.

### References

Faraway, Julian J. *Linear models with R*. CRC press, 2014.

Inoue, Takakatsu. "Improving the'HKB'ordinary type ridge estimator." *Journal of the Japan Statistical Society* 31.1 (2001): 67-83.

Brown, Philip J. Philip J. *Measurement, regression, and calibration*. No. 04; QA278. 2, B7.. 1993.

Hoerl, Arthur E., and Robert W. Kennard. "Ridge regression: Biased estimation for nonorthogonal problems." *Technometrics* 12.1 (1970): 55-67.

Hoerl, Arthur E., and Robert W. Kennard. "Ridge regression: applications to nonorthogonal problems." *Technometrics* 12.1 (1970): 69-82.

### See also

* [Package 'MASS'](https://cran.r-project.org/web/packages/MASS/MASS.pdf)
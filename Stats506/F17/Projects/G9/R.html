<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Wan-Lun,Tsai" />


<title>R Implementation</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/journal.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 61px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 66px;
  margin-top: -66px;
}

.section h2 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h3 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h4 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h5 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h6 {
  padding-top: 66px;
  margin-top: -66px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">SVM</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="R.html">R Implementation</a>
</li>
<li>
  <a href="python.html">Python Implementation</a>
</li>
<li>
  <a href="matlab.html">Matlab Implementation</a>
</li>
<li>
  <a href="references.html">References</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">R Implementation</h1>
<h4 class="author"><em>Wan-Lun,Tsai</em></h4>

</div>


<p>This tutorial is going to show the data analysis using Support Vector Machines, and the packages below will be used.</p>
<pre><code>library(data.table)
library(dplyr)
library(e1071)
library(caret)</code></pre>
<div id="description-of-data" class="section level3">
<h3>Description of data</h3>
<p>This data set was created by Max Little of the University of Oxford, in collaboration with the National Centre for Voice and Speech, Denver, Colorado. And it is composed of a range of biomedical voice measurements from 31 people, 23 with Parkinson’s disease (PD). Each column in the table is a particular voice measure, and each row corresponds one of 195 voice recording from these individuals (“name” column). The main aim of the data is to discriminate healthy people from those with PD, according to “status” column which is set to 0 for healthy and 1 for PD.</p>
<pre class="r"><code>data=fread(&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data&#39;)
head(data)</code></pre>
<pre><code>##              name MDVP:Fo(Hz) MDVP:Fhi(Hz) MDVP:Flo(Hz) MDVP:Jitter(%)
## 1: phon_R01_S01_1     119.992      157.302       74.997        0.00784
## 2: phon_R01_S01_2     122.400      148.650      113.819        0.00968
## 3: phon_R01_S01_3     116.682      131.111      111.555        0.01050
## 4: phon_R01_S01_4     116.676      137.871      111.366        0.00997
## 5: phon_R01_S01_5     116.014      141.781      110.655        0.01284
## 6: phon_R01_S01_6     120.552      131.162      113.787        0.00968
##    MDVP:Jitter(Abs) MDVP:RAP MDVP:PPQ Jitter:DDP MDVP:Shimmer
## 1:          0.00007  0.00370  0.00554    0.01109      0.04374
## 2:          0.00008  0.00465  0.00696    0.01394      0.06134
## 3:          0.00009  0.00544  0.00781    0.01633      0.05233
## 4:          0.00009  0.00502  0.00698    0.01505      0.05492
## 5:          0.00011  0.00655  0.00908    0.01966      0.06425
## 6:          0.00008  0.00463  0.00750    0.01388      0.04701
##    MDVP:Shimmer(dB) Shimmer:APQ3 Shimmer:APQ5 MDVP:APQ Shimmer:DDA     NHR
## 1:            0.426      0.02182      0.03130  0.02971     0.06545 0.02211
## 2:            0.626      0.03134      0.04518  0.04368     0.09403 0.01929
## 3:            0.482      0.02757      0.03858  0.03590     0.08270 0.01309
## 4:            0.517      0.02924      0.04005  0.03772     0.08771 0.01353
## 5:            0.584      0.03490      0.04825  0.04465     0.10470 0.01767
## 6:            0.456      0.02328      0.03526  0.03243     0.06985 0.01222
##       HNR status     RPDE      DFA   spread1  spread2       D2      PPE
## 1: 21.033      1 0.414783 0.815285 -4.813031 0.266482 2.301442 0.284654
## 2: 19.085      1 0.458359 0.819521 -4.075192 0.335590 2.486855 0.368674
## 3: 20.651      1 0.429895 0.825288 -4.443179 0.311173 2.342259 0.332634
## 4: 20.644      1 0.434969 0.819235 -4.117501 0.334147 2.405554 0.368975
## 5: 19.649      1 0.417356 0.823484 -3.747787 0.234513 2.332180 0.410335
## 6: 21.378      1 0.415564 0.825069 -4.242867 0.299111 2.187560 0.357775</code></pre>
<pre class="r"><code>names(data)</code></pre>
<pre><code>##  [1] &quot;name&quot;             &quot;MDVP:Fo(Hz)&quot;      &quot;MDVP:Fhi(Hz)&quot;    
##  [4] &quot;MDVP:Flo(Hz)&quot;     &quot;MDVP:Jitter(%)&quot;   &quot;MDVP:Jitter(Abs)&quot;
##  [7] &quot;MDVP:RAP&quot;         &quot;MDVP:PPQ&quot;         &quot;Jitter:DDP&quot;      
## [10] &quot;MDVP:Shimmer&quot;     &quot;MDVP:Shimmer(dB)&quot; &quot;Shimmer:APQ3&quot;    
## [13] &quot;Shimmer:APQ5&quot;     &quot;MDVP:APQ&quot;         &quot;Shimmer:DDA&quot;     
## [16] &quot;NHR&quot;              &quot;HNR&quot;              &quot;status&quot;          
## [19] &quot;RPDE&quot;             &quot;DFA&quot;              &quot;spread1&quot;         
## [22] &quot;spread2&quot;          &quot;D2&quot;               &quot;PPE&quot;</code></pre>
<pre class="r"><code>dim(data)</code></pre>
<pre><code>## [1] 195  24</code></pre>
<p>From the result above, we could check small part of the data, names of variales and the number of colnums and rows. Then we could know the variable “status” is going to be our response variable. Since the “name” column is not going to be neither a predictor or a rexponse variable, I am going to delete it.</p>
<p>And I also rearrange the order of columns to put the response in the last column.</p>
<pre class="r"><code>newdata=data%&gt;%select(-name)
newdata[,c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,19,20,21,22,23,17)]</code></pre>
<pre><code>##      MDVP:Fo(Hz) MDVP:Fhi(Hz) MDVP:Flo(Hz) MDVP:Jitter(%) MDVP:Jitter(Abs)
##   1:     119.992      157.302       74.997        0.00784          0.00007
##   2:     122.400      148.650      113.819        0.00968          0.00008
##   3:     116.682      131.111      111.555        0.01050          0.00009
##   4:     116.676      137.871      111.366        0.00997          0.00009
##   5:     116.014      141.781      110.655        0.01284          0.00011
##  ---                                                                      
## 191:     174.188      230.978       94.261        0.00459          0.00003
## 192:     209.516      253.017       89.488        0.00564          0.00003
## 193:     174.688      240.005       74.287        0.01360          0.00008
## 194:     198.764      396.961       74.904        0.00740          0.00004
## 195:     214.289      260.277       77.973        0.00567          0.00003
##      MDVP:RAP MDVP:PPQ Jitter:DDP MDVP:Shimmer MDVP:Shimmer(dB)
##   1:  0.00370  0.00554    0.01109      0.04374            0.426
##   2:  0.00465  0.00696    0.01394      0.06134            0.626
##   3:  0.00544  0.00781    0.01633      0.05233            0.482
##   4:  0.00502  0.00698    0.01505      0.05492            0.517
##   5:  0.00655  0.00908    0.01966      0.06425            0.584
##  ---                                                           
## 191:  0.00263  0.00259    0.00790      0.04087            0.405
## 192:  0.00331  0.00292    0.00994      0.02751            0.263
## 193:  0.00624  0.00564    0.01873      0.02308            0.256
## 194:  0.00370  0.00390    0.01109      0.02296            0.241
## 195:  0.00295  0.00317    0.00885      0.01884            0.190
##      Shimmer:APQ3 Shimmer:APQ5 MDVP:APQ Shimmer:DDA     NHR    HNR
##   1:      0.02182      0.03130  0.02971     0.06545 0.02211 21.033
##   2:      0.03134      0.04518  0.04368     0.09403 0.01929 19.085
##   3:      0.02757      0.03858  0.03590     0.08270 0.01309 20.651
##   4:      0.02924      0.04005  0.03772     0.08771 0.01353 20.644
##   5:      0.03490      0.04825  0.04465     0.10470 0.01767 19.649
##  ---                                                              
## 191:      0.02336      0.02498  0.02745     0.07008 0.02764 19.517
## 192:      0.01604      0.01657  0.01879     0.04812 0.01810 19.147
## 193:      0.01268      0.01365  0.01667     0.03804 0.10715 17.883
## 194:      0.01265      0.01321  0.01588     0.03794 0.07223 19.020
## 195:      0.01026      0.01161  0.01373     0.03078 0.04398 21.209
##          RPDE      DFA   spread1  spread2       D2      PPE status
##   1: 0.414783 0.815285 -4.813031 0.266482 2.301442 0.284654      1
##   2: 0.458359 0.819521 -4.075192 0.335590 2.486855 0.368674      1
##   3: 0.429895 0.825288 -4.443179 0.311173 2.342259 0.332634      1
##   4: 0.434969 0.819235 -4.117501 0.334147 2.405554 0.368975      1
##   5: 0.417356 0.823484 -3.747787 0.234513 2.332180 0.410335      1
##  ---                                                              
## 191: 0.448439 0.657899 -6.538586 0.121952 2.657476 0.133050      0
## 192: 0.431674 0.683244 -6.195325 0.129303 2.784312 0.168895      0
## 193: 0.407567 0.655683 -6.787197 0.158453 2.679772 0.131728      0
## 194: 0.451221 0.643956 -6.744577 0.207454 2.138608 0.123306      0
## 195: 0.462803 0.664357 -5.724056 0.190667 2.555477 0.148569      0</code></pre>
<p>Befor using this dataset to fit model directly, in order to test the accuracy of prediction, we can separate the dataset into two parts–test data and training data. And then we could use the training data to fit SVM model and use test data to test the accuracy od prediction of fitted model.</p>
<pre class="r"><code>set.seed(100) 
rowIndices=1 : nrow(newdata) 
sampleSize=0.8 *length(rowIndices)
trainingRows=sample (rowIndices, sampleSize)
trainingData=newdata[trainingRows, ]
testData=newdata[-trainingRows, ]
dim(trainingData)</code></pre>
<pre><code>## [1] 156  23</code></pre>
<pre class="r"><code>dim(testData)</code></pre>
<pre><code>## [1] 39 23</code></pre>
</div>
<div id="fit-svm" class="section level3">
<h3>Fit SVM</h3>
<p>The code below estimates the SVM by using the svm function. We need to notice one thing here. In order for this function to perform classification, we must encode the response as a factor variable.</p>
<pre class="r"><code>fit.svm=svm(factor(status)~.,data=trainingData)
summary(fit.svm)</code></pre>
<pre><code>## 
## Call:
## svm(formula = factor(status) ~ ., data = trainingData)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  1 
##       gamma:  0.04545455 
## 
## Number of Support Vectors:  74
## 
##  ( 30 44 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  0 1</code></pre>
<p>From the result above, we could know there are 74 support vectors, 30 in one class, 44 in the other.</p>
<p>Besides, we could also know there are some important arguments in this function. For the cost argument, it allows us to specify the cost of a viaolation to the margin. If it is small, then many support vectors would be on the margin or violate the margin. On the contrary, if it is large, the marigin would be narrow and few support vectors would be on the margin or violate the margin. In shrt, it is general penalizing parameter for C-classification.</p>
<p>The kernel argument is used in training and predicting, the options include linear, polynomial,radial basis and sigmoid.</p>
<p>The gamma argument is for a nonlinear SVM with a Gaussian radial basis function kernel.</p>
<p>The default option for kernel is radial, and in the following steps, I choose linear and polynomial respectively to compare the result.<br />
</p>
<pre class="r"><code>fit.linearsvm=svm(factor(status)~.,kernel=&quot;linear&quot;,data=trainingData)
summary(fit.linearsvm)</code></pre>
<pre><code>## 
## Call:
## svm(formula = factor(status) ~ ., data = trainingData, kernel = &quot;linear&quot;)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  1 
##       gamma:  0.04545455 
## 
## Number of Support Vectors:  50
## 
##  ( 24 26 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  0 1</code></pre>
<pre class="r"><code>fit.polysvm=svm(factor(status)~.,kernel=&quot;polynomial&quot;,data=trainingData)
summary(fit.polysvm)</code></pre>
<pre><code>## 
## Call:
## svm(formula = factor(status) ~ ., data = trainingData, kernel = &quot;polynomial&quot;)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  polynomial 
##        cost:  1 
##      degree:  3 
##       gamma:  0.04545455 
##      coef.0:  0 
## 
## Number of Support Vectors:  66
## 
##  ( 29 37 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  0 1</code></pre>
</div>
<div id="prediction" class="section level3">
<h3>Prediction</h3>
<p>In this part, we could use this model to test the accuracy of prediction by predicting test data.And the table function below is for creating the confusion matrix.</p>
<pre class="r"><code>predict.svm=predict(fit.svm,testData)
table(predict=predict.svm,truth=testData$status)</code></pre>
<pre><code>##        truth
## predict  0  1
##       0  4  1
##       1  6 28</code></pre>
<p>From the output above, we could know there are only seven cases are predicted incorrectly, others are all right. Thus, in my opinion, this model is not bad for prediction.</p>
<p>Except the table function, we could also use confusionMatrix function to get more details.</p>
<pre class="r"><code>confusionMatrix(predict.svm,testData$status)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  0  1
##          0  4  1
##          1  6 28
##                                           
##                Accuracy : 0.8205          
##                  95% CI : (0.6647, 0.9246)
##     No Information Rate : 0.7436          
##     P-Value [Acc &gt; NIR] : 0.1808          
##                                           
##                   Kappa : 0.4371          
##  Mcnemar&#39;s Test P-Value : 0.1306          
##                                           
##             Sensitivity : 0.4000          
##             Specificity : 0.9655          
##          Pos Pred Value : 0.8000          
##          Neg Pred Value : 0.8235          
##              Prevalence : 0.2564          
##          Detection Rate : 0.1026          
##    Detection Prevalence : 0.1282          
##       Balanced Accuracy : 0.6828          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
</div>
<div id="optimal-parameters" class="section level3">
<h3>Optimal Parameters</h3>
<p>We can also use the tune.svm function to find the optimal parameters to minialize errors.</p>
<pre class="r"><code>tuned=tune.svm(factor(status) ~., data = trainingData,gamma = 10^(-8:1), cost = 10^(-2:2))
summary (tuned)</code></pre>
<pre><code>## 
## Parameter tuning of &#39;svm&#39;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  gamma cost
##    0.1   10
## 
## - best performance: 0.07708333 
## 
## - Detailed performance results:
##    gamma  cost      error dispersion
## 1  1e-08 1e-02 0.24416667 0.10233011
## 2  1e-07 1e-02 0.24416667 0.10233011
## 3  1e-06 1e-02 0.24416667 0.10233011
## 4  1e-05 1e-02 0.24416667 0.10233011
## 5  1e-04 1e-02 0.24416667 0.10233011
## 6  1e-03 1e-02 0.24416667 0.10233011
## 7  1e-02 1e-02 0.24416667 0.10233011
## 8  1e-01 1e-02 0.24416667 0.10233011
## 9  1e+00 1e-02 0.24416667 0.10233011
## 10 1e+01 1e-02 0.24416667 0.10233011
## 11 1e-08 1e-01 0.24416667 0.10233011
## 12 1e-07 1e-01 0.24416667 0.10233011
## 13 1e-06 1e-01 0.24416667 0.10233011
## 14 1e-05 1e-01 0.24416667 0.10233011
## 15 1e-04 1e-01 0.24416667 0.10233011
## 16 1e-03 1e-01 0.24416667 0.10233011
## 17 1e-02 1e-01 0.24416667 0.10233011
## 18 1e-01 1e-01 0.24416667 0.10233011
## 19 1e+00 1e-01 0.24416667 0.10233011
## 20 1e+01 1e-01 0.24416667 0.10233011
## 21 1e-08 1e+00 0.24416667 0.10233011
## 22 1e-07 1e+00 0.24416667 0.10233011
## 23 1e-06 1e+00 0.24416667 0.10233011
## 24 1e-05 1e+00 0.24416667 0.10233011
## 25 1e-04 1e+00 0.24416667 0.10233011
## 26 1e-03 1e+00 0.24416667 0.10233011
## 27 1e-02 1e+00 0.12208333 0.05846711
## 28 1e-01 1e+00 0.10250000 0.06261409
## 29 1e+00 1e+00 0.18625000 0.10952074
## 30 1e+01 1e+00 0.24416667 0.10233011
## 31 1e-08 1e+01 0.24416667 0.10233011
## 32 1e-07 1e+01 0.24416667 0.10233011
## 33 1e-06 1e+01 0.24416667 0.10233011
## 34 1e-05 1e+01 0.24416667 0.10233011
## 35 1e-04 1e+01 0.24416667 0.10233011
## 36 1e-03 1e+01 0.12208333 0.05846711
## 37 1e-02 1e+01 0.12208333 0.07179494
## 38 1e-01 1e+01 0.07708333 0.06687612
## 39 1e+00 1e+01 0.18000000 0.11121176
## 40 1e+01 1e+01 0.21833333 0.10313751
## 41 1e-08 1e+02 0.24416667 0.10233011
## 42 1e-07 1e+02 0.24416667 0.10233011
## 43 1e-06 1e+02 0.24416667 0.10233011
## 44 1e-05 1e+02 0.24416667 0.10233011
## 45 1e-04 1e+02 0.12208333 0.05846711
## 46 1e-03 1e+02 0.12208333 0.07179494
## 47 1e-02 1e+02 0.10958333 0.06158434
## 48 1e-01 1e+02 0.07708333 0.06687612
## 49 1e+00 1e+02 0.18000000 0.11121176
## 50 1e+01 1e+02 0.21833333 0.10313751</code></pre>
<p>From the result above, we can know in this case, the optimal parameters for gamma and cost are 0.1 and 10, respectively.</p>
<p>Then , we can use this result to fit the model and do the predictation again.</p>
<pre class="r"><code>fit.tuned=svm(factor(status)~.,data=trainingData,gamma=0.1,cost=10)
predict.tuned=predict(fit.tuned,testData)
confusionMatrix(predict.tuned,testData$status)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  0  1
##          0 10  1
##          1  0 28
##                                           
##                Accuracy : 0.9744          
##                  95% CI : (0.8652, 0.9994)
##     No Information Rate : 0.7436          
##     P-Value [Acc &gt; NIR] : 0.0001386       
##                                           
##                   Kappa : 0.9349          
##  Mcnemar&#39;s Test P-Value : 1.0000000       
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.9655          
##          Pos Pred Value : 0.9091          
##          Neg Pred Value : 1.0000          
##              Prevalence : 0.2564          
##          Detection Rate : 0.2564          
##    Detection Prevalence : 0.2821          
##       Balanced Accuracy : 0.9828          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p>In the first model, the accuracy of predictation is 0.82. After choosing the optimal parameters, the accuracy of predictation rises to 0.97.</p>
<pre class="r"><code>proc.time()</code></pre>
<pre><code>##    user  system elapsed 
##   18.59    1.87   29.96</code></pre>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
